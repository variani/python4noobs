## Large data

>  I'm not talking about "big data" that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.

## References

Discussions

* http://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas: suggested HDFStore, Ruffus (bioinformatics apps), Blaze
* http://stackoverflow.com/questions/24007762/python-pandas-using-to-sql-to-write-large-data-frames-in-chunks: parse csv to SQL by chunks

Libraries

* http://docs.ibis-project.org/sql.html
